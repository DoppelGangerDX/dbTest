# Databricks notebook source
# MAGIC %md ## Importing Libraries

# COMMAND ----------

import os
import numpy as np
import pandas as pd
import psycopg2 as pg2
from datetime import datetime
from dateutil.parser import parse
from datetime import timedelta
import pytz
import sys

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("skyhook").getOrCreate()
from pyspark.sql.types import StructField, StringType, IntegerType, FloatType, StructType, ArrayType
from pyspark.sql.functions import length, size, col
from pyspark.sql.functions import udf, lit

# COMMAND ----------

# country	region	minLat	maxLat	minLon	maxLon
# usa	chicago	41.642534	42.023939	-87.945888	-87.522571
# uk	london	51.25221	51.722526	-0.552882	0.298559
# netherlands	complete	50.74977	53.512552	3.360316	7.06985
# zambia complete -18.098574	-8.192271	21.981164	33.712916
# canada complete  41.684297	83.095765	-140.991663	-52.622197
# japan tokyo 35.099698	36.261724	138.897648	140.266909
# singapore complete 1.186997	1.482174	103.572327	104.091431
# usa custom sohrab
# uk	complete	49.859579	59.439761	-10.817547	1.838702 - Hong
# Vietnam 8.563684	23.392975	102.143999	109.467623
#uk london for test 51.23 51.74 -0.58 0.32
# saudi Arabia 16.378435 32.179842 34.583249 55.646325
#peru -18.354572	-0.036865	-81.323658	-68.669823
#France 42.350036	51.088311	-4.794957	7.716146
#Belgium 49.497821	51.503273	2.546331	6.405566
#Portugal 36.993584	42.149907	-9.49896	-6.19121
#Spain 36.002077	43.393083	-9.298644	3.314152
#China top 39.38584, 53.59, 73.51684, 134.7635
#China left 18.16,  39.385841, 73.516837, 104.942832
#China right 18.16, 40.895914,   104.942832, 123.817815
#Germany 47.270358, 54.906365, 5.869131, 15.041012



# COMMAND ----------

# MAGIC %md ### User Inputs : Geography and Timelines

# COMMAND ----------

## User Inputs

# Country & Region Name

country = "Germany"             #### Input 1 #### #Please don't use spaces
region = "Complete"              #### Input 2 ####   #Please don't use spaces
#country = getArgument("country", "test1") 
#region = getArgument("region", "test123") 


# Bounding Box Lat/Lon Values

minLat, maxLat, minLon, maxLon = (47.270358, 54.906365, 5.869131, 15.041012)       #### Input 3 ####

# Start/End Date Inputs

start_date, end_date = ("01/01/2017" , "12/31/2017")           #### Input 4 ####

# COMMAND ----------

# Consolidating Inputs

bounding_box = (minLat, maxLat, minLon, maxLon)

geography = [country, region, bounding_box]

timelines = [start_date, end_date]

# COMMAND ----------

# Python function to verify inputs

# Geography
# country = geography[0].lower()
# region = geography[1].lower()
# bounding_box = geography[2]
  
print("Country: %s" %country)
print("Region: %s" %region)
#print("Bounding Box: ", bounding_box)
  
# Timelines
#start_date = timelines[0]
#end_date = timelines[1]
  
try:
  start_date = datetime.strptime(start_date, '%m/%d/%Y')
  print("Processing Start Date : %s" %(start_date))

except:
  print("Date Input Error!! Start Date defaulted to 01/01/2017")
    
try:
  end_date = datetime.strptime(end_date, '%m/%d/%Y')
  print("Processing End Date : %s" %(end_date))

except:
  print("Date Input Error!! End Date defaulted to 12/31/2017")
  
print("User Inputs Succesful")

# COMMAND ----------

# Importing All TileID's

allTileBBoxes = spark.read.parquet("s3://mck-67-output/tileIDs/allTileBBoxes")

# There are about 68 million unique tileID's in the given dataset

# Filter Tile ID's to the Geography of Interest

tileBBoxes = allTileBBoxes.filter('(minLat BETWEEN %s AND %s) AND (minLon BETWEEN %s AND %s)'%(minLat, maxLat, minLon, maxLon))   # Filtering geographies

# COMMAND ----------

tiles = tileBBoxes.select('tileID')

# COMMAND ----------

tiles_allhours = tiles.withColumn('hour', lit(0))

for i in range(0,24):
    
    tiles_hour = tiles.withColumn('hour', lit(i))
    
    tiles_allhours = tiles_allhours.union(tiles_hour)
    
tiles_allhours = tiles_allhours.sort(['tileID', 'hour'])

# COMMAND ----------

tiles_allhours = tiles_allhours.withColumn('mon', lit(0))
tiles_allhours = tiles_allhours.withColumn('tue', lit(0))
tiles_allhours = tiles_allhours.withColumn('wed', lit(0))
tiles_allhours = tiles_allhours.withColumn('thu', lit(0))
tiles_allhours = tiles_allhours.withColumn('fri', lit(0))
tiles_allhours = tiles_allhours.withColumn('sat', lit(0))
tiles_allhours = tiles_allhours.withColumn('sun', lit(0))

tilesOutputTable = tiles_allhours

# COMMAND ----------

# Useful Data Dictionaries

weekhours = {0 : (24,47), 1 : (48,71), 2 : (72,95), 3 : (96,119), 4 : (120,143), 5 : (144,167), 6 : (0,23)}

dayofweek_dict = {0:'mon', 1:'tue', 2:'wed', 3:'thu', 4:'fri', 5:'sat', 6:'sun'}

# COMMAND ----------

# Input Data Schema

input_data_schema = [StructField('tileID', StringType(), True),
                     StructField('hour168', IntegerType(), True),
                     StructField('type', IntegerType(), True),
                     StructField('pings', IntegerType(), True),
                     StructField('avgLat', FloatType(), True),
                     StructField('avgLon', FloatType(), True)]

final_struct = StructType(fields=input_data_schema)

# COMMAND ----------

day_count = {'mon':0, 'tue':0, 'wed':0, 'thu':0, 'fri':0, 'sat':0, 'sun':0}

for day in range((end_date - start_date).days+1):
  
    current_date = start_date + timedelta(day)

    dayofweek = dayofweek_dict[current_date.weekday()]
    
#     # Missing Data
    
#     if str(current_date) == "2017-10-27 00:00:00":
        
#         print(str(current_date), "||Watch out for this")
#         continue
    
#     ###############
    
    # Loading Data
    
    input_path = "s3://mck-67-data/original/%s/%s/" %(str(current_date.year), str(current_date.date()))
    dayData = spark.read.csv(input_path, sep="\t", header='false', schema=final_struct)
    
    print("Loading data from ", input_path)
    
    # Filtering Data
    
    dayData = dayData.filter('type IN (101, 102, 103, 104, 106, 203, 204, 206)') # Filtering Ping Types
    dayData = dayData.filter(length(dayData.tileID) == 9)  # Filtering tiles by sizes
    dayData = dayData.filter('hour168 BETWEEN %s AND %s' %(str(weekhours[current_date.weekday()][0]), str(weekhours[current_date.weekday()][1]))) # Filtering hours of the day
    dayData = dayData.filter('(avgLat BETWEEN %s AND %s)	AND (avgLon BETWEEN %s AND %s)'%(minLat, maxLat, minLon, maxLon))   # Filtering geographies
    
    # Normalizing Hour of th# e day
    
    dayData = dayData.withColumn('hour168', dayData.hour168 % 24)
    
    
    # Grouping different ping types and Selecting relevant columns
    
    dayData = dayData.groupBy(['tileID', 'hour168']).sum()
    
    dayData = dayData.select('tileID', col('hour168').alias('hour'), col('sum(pings)').alias('pings_dyn'))
    
    
    # Joining each day's data to the output table
    
    tilesOutputTable = tilesOutputTable.join(dayData, on=['tileID', 'hour'], how='left')
    
    tilesOutputTable = tilesOutputTable.na.fill(0)
    
    tilesOutputTable = tilesOutputTable.withColumn(dayofweek, tilesOutputTable[dayofweek] + tilesOutputTable['pings_dyn'])
    
    tilesOutputTable = tilesOutputTable.select('tileID', 'hour', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun')
    
    day_count[dayofweek] += 1
    
    print(str(weekhours[current_date.weekday()][0]), str(weekhours[current_date.weekday()][1]))

# COMMAND ----------

day_count

# COMMAND ----------

### Do Not Run this ### It will overwrite the data.

pings_path = "s3://mck-67-output/SparkExtracts1/%s/%s_%s_Pings" %(country, country, region)
bboxes_path = "s3://mck-67-output/SparkExtracts1/%s/%s_%s_BBoxes" %(country, country, region)

tilesOutputTable.repartition(1).write.mode("overwrite").csv(pings_path, header=True)
tileBBoxes.repartition(1).write.mode("overwrite").csv(bboxes_path, header=True)

# COMMAND ----------

pings_path